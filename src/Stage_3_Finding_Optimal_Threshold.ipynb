{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CZ4041 Group 28 Notebook 3\n",
    "To Run this Notebook, Import the .csv Files from the Notebook 2<br>\n",
    "The .csv Files are:\n",
    "\n",
    "- train_missing_count.csv \n",
    "- test_missing_count.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.applications import DenseNet121\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "step2_train = pd.read_csv(\"../input/stage2/train_missing_count.csv\").sort_values(by=[\"ImageId\"])\n",
    "\n",
    "# Reset Index, as Sorting Values by ImageId Messes Up Index\n",
    "step2_train.reset_index(inplace = True, drop = True)\n",
    "# Replace the .png extension to .jpg\n",
    "step2_train[\"ImageId\"] = step2_train[\"ImageId\"].str.replace(\"png\",\"jpg\")\n",
    "\n",
    "step2_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/severstal-steel-defect-detection/train.csv\").sort_values(by=['ImageId'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_train.info()\n",
    "print()\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataframes for EDA\n",
    "We will First Create the final_train_df. \n",
    "\n",
    "final_train_df will be the Main Dataframe Used/Manipulated from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Merge the train_df and step2_train\n",
    "final_train_df = train_df.merge(step2_train, on='ImageId', how='right').sort_values(by='ImageId').reset_index(drop=True)\n",
    "final_train_df\n",
    "\n",
    "# For Downloading final_train_df csv, if needed.\n",
    "# final_train_df.to_csv('final_train_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that here is 12997-429 = 12568 (Total Number of Images)\n",
    "# 5902 + 6239 + 425 + 2 = 12568, and\n",
    "# 5902 + 6239 + 425*2 + 2*3 = 12997 (Total Number of Entries)\n",
    "\n",
    "data =  final_train_df[\"ClassId\"].groupby(final_train_df[\"ImageId\"]).count()\n",
    "plt.figure(figsize=(5,7))\n",
    "plt.suptitle(\"Number of Defect Classes per Image\")\n",
    "plt.grid()\n",
    "\n",
    "ax = sns.countplot(data)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlabel(\"No. of Defects\")\n",
    "\n",
    "print(\"Numeric info : \\n\")\n",
    "print(\"0 Defects: \",len(data[data == 0]))\n",
    "print(\"1 Defect : \",len(data[data == 1]))\n",
    "print(\"2 Defects: \",len(data[data == 2]))\n",
    "print(\"3 Defects: \",len(data[data == 3]))\n",
    "print(\"4 Defects: \",len(data[data == 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to note that at this point of the pipeline, images that are **not defects but has been classified as a defect** do not have a ClassId tied to them. \n",
    "\n",
    "As such, the EDA we just performed only utilizes images with a ClassId."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shall investigate what value shall be our threshold for the allMissing scoring system we implemented. An allMissing score below the threshold value will be treated as a defective image.\n",
    "\n",
    "The ideal threshold score will maximize the correct predictions for defects, and minimize misclassification of non-defect images. The three usable metrics to be considered are:\n",
    "\n",
    "F1 Score = $ 2*(Recall * Precision) \\over (Recall + Precision)$\n",
    "\n",
    "Recall = $ TP \\over (TP+FN)$\n",
    "\n",
    "Precision = $ TP \\over (TP + FP)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df.loc[final_train_df['ClassId'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df.loc[final_train_df['ClassId'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getnondefect(threshold):\n",
    "    return (len(final_train_df.loc[(final_train_df['ClassId'].isnull()) & (final_train_df['allMissing'] > threshold)])/5902)\n",
    "\n",
    "def getdefect(threshold):\n",
    "    return (len(final_train_df.loc[(final_train_df['ClassId'].notnull()) & (final_train_df['allMissing'] < threshold)])/7095)\n",
    "\n",
    "def TP(threshold):\n",
    "    return (len(final_train_df.loc[(final_train_df['ClassId'].notnull()) & (final_train_df['allMissing'] < threshold)])/7095)*100\n",
    "\n",
    "def FN(threshold):\n",
    "    return (len(final_train_df.loc[(final_train_df['ClassId'].notnull()) & (final_train_df['allMissing'] > threshold)])/7095)*100\n",
    "\n",
    "def FP(threshold):\n",
    "    return (len(final_train_df.loc[(final_train_df['ClassId'].isnull()) & (final_train_df['allMissing'] < threshold)])/7095)*100\n",
    "\n",
    "def TN(threshold):\n",
    "    return (len(final_train_df.loc[(final_train_df['ClassId'].isnull()) & (final_train_df['allMissing'] > threshold)])/5902)*100\n",
    "\n",
    "def recall(TP, FN):\n",
    "    return (TP/(TP+FN))\n",
    "\n",
    "def precision(TP, FP):\n",
    "    return TP/(TP+FP)\n",
    "\n",
    "def F1(Recall, Precision):\n",
    "    return (2*(Recall*Precision))/(Recall+Precision)\n",
    "\n",
    "def accuracy(TP, TN, FP, FN):\n",
    "    return (TP+TN)/(TP+TN+FP+FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dataframe to Compare Threshold Value to Non-Defect Images (% of them Below Threshold Value)\n",
    "\n",
    "threshold_setting = pd.DataFrame(float(i/1000) for i in range(0, 1001))\n",
    "threshold_setting = threshold_setting.drop([0,1000])\n",
    "\n",
    "threshold_setting['isnull'] = threshold_setting[0].apply(getnondefect)\n",
    "threshold_setting['Defective Images (%) Below Threshold'] = threshold_setting[0].apply(getdefect)\n",
    "\n",
    "threshold_setting['TP'] = threshold_setting[0].apply(TP)\n",
    "threshold_setting['TN'] = threshold_setting[0].apply(TN)\n",
    "threshold_setting['FN'] = threshold_setting[0].apply(FN)\n",
    "threshold_setting['FP'] = threshold_setting[0].apply(FP)\n",
    "\n",
    "threshold_setting['Accuracy'] = threshold_setting.apply(lambda x: accuracy(x['TP'], x['TN'], x['FP'], x['FN']), axis = 1)\n",
    "threshold_setting['Recall'] = threshold_setting.apply(lambda x: recall(x['TP'], x['FN']), axis = 1)\n",
    "threshold_setting['Precision'] = threshold_setting.apply(lambda x: precision(x['TP'], x['FP']), axis = 1)\n",
    "threshold_setting['F1'] = threshold_setting.apply(lambda x: F1(x['Recall'], x['Precision']), axis = 1)\n",
    "\n",
    "# Renaming Column names\n",
    "threshold_setting.columns = [\"Threshold Value\", \"Non-Defect Images (%) Above Threshold\", 'Defective Images (%) Below Threshold',\n",
    "                            \"TP\", \"TN\", \"FN\", \"FP\", \"Accuracy\", \"Recall\", \"Precision\", \"F1\"]\n",
    "# threshold_setting = threshold_setting.drop([0, 1000])\n",
    "\n",
    "threshold_setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(threshold_setting)\n",
    "\n",
    "fig.add_scatter(x=threshold_setting[\"Threshold Value\"], y=threshold_setting['Non-Defect Images (%) Above Threshold'], name=\"Non-Defect Images (%) Above Threshold\")\n",
    "fig.add_scatter(x=threshold_setting[\"Threshold Value\"], y=threshold_setting['Defective Images (%) Below Threshold'], name=\"Defective Images (%) Below Threshold\")\n",
    "\n",
    "fig.update_layout(title = \"% Defective/Non Defective Images\",\n",
    "                 xaxis_title = \"Threshold Value\"\n",
    "                 )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting this graph allows us to see that perhaps at the intersect of both line would be our ideal threshold value. This intersect marks the threshold value that yields the highest combined number of defect and non-defect classification.\n",
    "\n",
    "However,our pipeline should punish False Positives as images classified as defective despite being non-defective are heavily penalized in the competition. Therefore we should consider using the Precision to determine our threshold value.\n",
    "\n",
    "Recall = $ TP \\over (TP+FP)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Visualize How the Behaviours of our Various Metrics with Regards to the Threshold Value\n",
    "fig = px.line(threshold_setting)\n",
    "\n",
    "fig.add_scatter(x=threshold_setting[\"Threshold Value\"], y=threshold_setting['Accuracy'], name=\"Accuracy\")\n",
    "fig.add_scatter(x=threshold_setting[\"Threshold Value\"], y=threshold_setting['Precision'], name=\"Precision\")\n",
    "fig.add_scatter(x=threshold_setting[\"Threshold Value\"], y=threshold_setting['Recall'], name=\"Recall\")\n",
    "fig.add_scatter(x=threshold_setting[\"Threshold Value\"], y=threshold_setting['F1'], name=\"F1\")\n",
    "\n",
    "fig.update_layout(title = \"Accuracy, Recall, Precision, F1 vs Threshold Value\",\n",
    "                 xaxis_title = \"Threshold Value\"\n",
    "                 )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to highlight that the Precision alone would be a tough metric to use, as the graph shows that maximizing Precision will result in a threshold value close to 0, and all images are considered defects. Also, the Accuracy, Precision and F1 are at their minimum when threshold value is set to 0.\n",
    "\n",
    "As such, we will have to use a metric that combines both Recall and Precision. Since the F1 Score utilizes both Precision and Recall, we shall investigate the use of F1 score as the metric to decide on the threshold value.\n",
    "\n",
    "F1 Score = $ 2*(Recall * Precision) \\over (Recall + Precision)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we Find the Row with the Highest F1 Score Attained. Included in the Row would be the Various Metrics.\n",
    "F1 = list(threshold_setting['F1'])\n",
    "F1.sort()\n",
    "threshold_setting.loc[threshold_setting['F1'] == F1[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Threshold Value where Accuracy, Recall, Precision and F1 are almost the Same (with reference to the graph)\n",
    "\n",
    "threshold_setting.loc[abs(threshold_setting['Accuracy']-threshold_setting['Recall'])\n",
    "                      + abs(threshold_setting['Recall']-threshold_setting['Precision'])\n",
    "                      + abs(threshold_setting['Precision']-threshold_setting['F1'])<0.003]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use the threshold value of 0.919 as it resulted in the most consistent score for all four metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Now can Apply the Threshold Score, 0.919 to our Dataset\n",
    "THRESHOLD = 0.919\n",
    "class1 = final_train_df.loc[(final_train_df['ClassId'] == 1) & (final_train_df['allMissing'] < THRESHOLD)]\n",
    "class2 = final_train_df.loc[(final_train_df['ClassId'] == 2) & (final_train_df['allMissing'] < THRESHOLD)]\n",
    "class3 = final_train_df.loc[(final_train_df['ClassId'] == 3) & (final_train_df['allMissing'] < THRESHOLD)]\n",
    "class4 = final_train_df.loc[(final_train_df['ClassId'] == 4) & (final_train_df['allMissing'] < THRESHOLD)]\n",
    "\n",
    "print(\"Class 1 Defects: \" + str(len(final_train_df.loc[(final_train_df['ClassId'] == 1)]))\n",
    "     + \"\\nClass 2 Defects: \" + str(len(final_train_df.loc[(final_train_df['ClassId'] == 2)]))\n",
    "     + \"\\nClass 3 Defects: \" + str(len(final_train_df.loc[(final_train_df['ClassId'] == 3)]))\n",
    "     + \"\\nClass 4 Defects: \" + str(len(final_train_df.loc[(final_train_df['ClassId'] == 4)]))\n",
    "      \n",
    "     + \"\\n\\nClass 1 Defects Predicted: \" + str(len(class1))\n",
    "     + \"\\nClass 2 Defects Predicted: \" + str(len(class2))\n",
    "     + \"\\nClass 3 Defects Predicted: \" + str(len(class3))\n",
    "     + \"\\nClass 4 Defects Predicted: \" + str(len(class4))\n",
    "     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
